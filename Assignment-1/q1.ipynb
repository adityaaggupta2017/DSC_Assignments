{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded the \"AUTO MPG\" from the excel file mentioned in the question . Now we can apply changes as asked in the questions . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PART A \n",
    "\n",
    "For discrete attributes, apply a one-hot encoding, and for non numeric ordinal\n",
    "attributes, apply integer mapping. Handle outliers/noise and save this in a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the question , we need to apply one-hot encoding to the dataset for the discrete attributes . Now we analyzed the features given in the dataset and get to know that cylinders , model year and  origin . Thus we need to first apply one hot encoding to these . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 4 6 3 5]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Assignment-1 - Auto MPG.csv\")\n",
    "\n",
    "# lets get the unique values of the cylinders first . \n",
    "\n",
    "unique_cylinder_values = dataset['cylinders'].unique() ; \n",
    "\n",
    "print(unique_cylinder_values) ; \n",
    "\n",
    "# now we can add new columns to the dataset for each discrete value and if the entry has that no of cylinders the column corresponding to it will become 1 else 0 ; \n",
    "\n",
    "for val in unique_cylinder_values: \n",
    "    if (pd.notna(val)) :\n",
    "        dataset[f'cylinder_{val}'] = 0 \n",
    "        for i in range(len(dataset)):  \n",
    "            if dataset.loc[i, 'cylinders'] == val:\n",
    "                dataset.loc[i, f'cylinder_{val}'] = 1 \n",
    "  \n",
    "# thus if number of cylinders for a entry is 4 then cylinder_4 column will be 1 for it otherwise it will be 0 . \n",
    "\n",
    "# similiarly we can do for the origin values as follows : \n",
    "\n",
    "unique_origin_values = dataset['origin'].unique() ; \n",
    "\n",
    "for val in unique_origin_values:\n",
    "    if (pd.notna(val)):\n",
    "        dataset[f'origin_{val}'] = 0 \n",
    "        for i in range(len(dataset)):  \n",
    "            if dataset.loc[i, 'origin'] == val:\n",
    "                dataset.loc[i, f'origin_{val}'] = 1  \n",
    "\n",
    "unique_model_year_values = dataset['model year'].unique() ; \n",
    "\n",
    "for val in unique_model_year_values:\n",
    "    if (pd.notna(val)):\n",
    "        dataset[f'model year_{val}'] = 0 \n",
    "        for i in range(len(dataset)):  \n",
    "            if dataset.loc[i, 'model year'] == val:\n",
    "                dataset.loc[i, f'model year_{val}'] = 1  \n",
    "                \n",
    "dataset.drop(['cylinders', 'origin' , 'model year'], axis=1, inplace=True)  ; \n",
    "\n",
    "# thus we changed the discrete attributes with their one-hot encoding . \n",
    "# Now we need to apply integer mapping for non - numeric ordinal attributes . \n",
    "\n",
    "# We dont have any such attribute so we can skip this step . Check csv file for the same . \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are 0 non - numeric ordinal attributes . Thus we can skip the step of applying integer mapping to such values . Now we need to handle outliers/noise and save this in a file . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_29628\\1747376723.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataset[column].fillna(median_val , inplace = True) ;\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_29628\\1747376723.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataset[column].fillna(median_val , inplace = True) ;\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_29628\\1747376723.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[column].fillna(median_val , inplace = True) ;\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_29628\\1747376723.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataset[column].fillna(median_val , inplace = True) ;\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_29628\\1747376723.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataset[column].fillna(median_val , inplace = True) ;\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_29628\\1747376723.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataset[column].fillna(median_val , inplace = True) ;\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_29628\\1747376723.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataset[column].fillna(mode_val , inplace = True) ;\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean(data) : \n",
    "  if (len(data) == 0):\n",
    "    return None ; \n",
    "  \n",
    "  return sum(data) / len(data) ; \n",
    "\n",
    "def calculate_median(data):\n",
    "    sorted_data = sorted(data)\n",
    "    n = len(sorted_data)\n",
    "    \n",
    "    if n == 0:\n",
    "        return None\n",
    "    \n",
    "    if n % 2 == 1:\n",
    "        return sorted_data[n // 2]\n",
    "    middle1 = sorted_data[n // 2 - 1]\n",
    "    middle2 = sorted_data[n // 2]\n",
    "    return (middle1 + middle2) / 2\n",
    "  \n",
    "def calculate_mode(data):\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    counter = Counter(data)\n",
    "    mode_data = counter.most_common(1)\n",
    "    return mode_data[0][0] if mode_data else None\n",
    "\n",
    "dataset['horsepower'] = pd.to_numeric(dataset['horsepower'] , errors='coerce') ; \n",
    "\n",
    "numerical_cols = ['mpg', 'displacement', 'horsepower', 'weight', 'acceleration']\n",
    "\n",
    "# below is the implementation for my numerical value\n",
    "def fill_missing_numerical(dataset, column):\n",
    "  mean_val = calculate_mean(dataset[column].dropna()) ; \n",
    "  median_val = calculate_median(dataset[column].dropna()) ; \n",
    "  \n",
    "  if (dataset[column].skew() > 0.5 or dataset[column].skew() < 0.5):\n",
    "    \n",
    "    dataset[column].fillna(median_val , inplace = True) ; \n",
    "    \n",
    "  else:\n",
    "    dataset[column].fillna(mean_val , inplace = True) ; \n",
    "\n",
    "# below is the implementation for my one-hot encoded/categorical value\n",
    "def fill_missing_categorical(dataset , column): \n",
    "  mode_val = calculate_mode(dataset[column].dropna()) ;\n",
    "  \n",
    "  dataset[column].fillna(mode_val , inplace = True) ; \n",
    "  \n",
    "  \n",
    "# i am using IQR as taught in class\n",
    "def remove_outliers(dataset ,  column):\n",
    "  q1 = dataset[column].quantile(0.25) ; \n",
    "  \n",
    "  q3 = dataset[column].quantile(0.75) ; \n",
    "  \n",
    "  iqr = q3 - q1 ; \n",
    "  \n",
    "  lower_bound = q1 - 1.5 * iqr ; \n",
    "  upper_bound = q3 + 1.5 * iqr ; \n",
    "  \n",
    "  \n",
    "  dataset = dataset[(dataset[column] >= lower_bound) & (dataset[column] <= upper_bound) ] ; \n",
    "  \n",
    "  \n",
    "  return dataset ; \n",
    "def clean_data(dataset , numerical_cols):\n",
    "  for column in dataset.columns:\n",
    "    if (column in numerical_cols):\n",
    "      fill_missing_numerical(dataset , column ) ; \n",
    "      \n",
    "      dataset = remove_outliers(dataset , column ) ; \n",
    "    \n",
    "    else: \n",
    "      fill_missing_categorical(dataset , column) ;\n",
    "      \n",
    "  \n",
    "  return dataset ; \n",
    "\n",
    "\n",
    "dataset_new = clean_data(dataset , numerical_cols) ; \n",
    "\n",
    "\n",
    "      \n",
    "dataset_new.to_csv('Cleaned_Dataset.csv' , index=False) ; \n",
    "      \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part b \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the question , we need to calculate the mean x bar and variance sigma^2 of the data in it . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we have datapoints with d number of features in them . Thus the mean x bar will be of the form d*1 , where d is the dimension(number of features of each datapoint .) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.36685185e+01 1.87691799e+02 1.01492063e+02 2.93610714e+03\n",
      " 1.56063492e+01 2.38095238e-01 5.21164021e-01 2.22222222e-01\n",
      " 1.05820106e-02 7.93650794e-03 6.19047619e-01 2.06349206e-01\n",
      " 1.71957672e-01 5.29100529e-02 7.14285714e-02 6.87830688e-02\n",
      " 9.78835979e-02 7.14285714e-02 7.93650794e-02 8.73015873e-02\n",
      " 7.40740741e-02 9.52380952e-02 7.14285714e-02 6.87830688e-02\n",
      " 2.64550265e-03 7.67195767e-02 7.93650794e-02]\n"
     ]
    }
   ],
   "source": [
    "dataset_cleaned = pd.read_csv(\"Cleaned_Dataset.csv\") ; \n",
    "\n",
    "# now the dataset_cleaned contains the datapoints with the one-hot encoded features (thus there are now 14 columns for each datapoint) . \n",
    "# the mean is as follows : \n",
    "\n",
    "# converting the dataframe into numpy array for easier manupilation and calculation . \n",
    "\n",
    "dataset_cleaned = dataset_cleaned.to_numpy() ; \n",
    "\n",
    "#print(dataset)\n",
    "\n",
    "# Now lets calculate the mean of the values . \n",
    "\n",
    "no_of_datapoint , no_of_features = dataset_cleaned.shape ; \n",
    "\n",
    "\n",
    "mean_x = np.zeros(no_of_features) ; # as the array of x bar will be of the shape d * 1 ; \n",
    "\n",
    "# print(dataset_cleaned)\n",
    "for i in range(no_of_datapoint):\n",
    "  \n",
    "  \n",
    "  mean_x += dataset_cleaned[i] ; # this way i am adding the ith dimensional feature to the ith index in the mean_x array . \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean_x /= no_of_datapoint ; \n",
    "\n",
    "print(mean_x) \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the variance of the dataset using the features and the transpose formulae given in the question . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684779.143322625\n"
     ]
    }
   ],
   "source": [
    "# now we have dataset_cleaned as a numpy array so we can calculate the variance as follows : \n",
    "\n",
    "# mean_X is the mean of X ie X bar \n",
    "\n",
    "variance = 0 ; \n",
    "\n",
    "for i in range(no_of_datapoint) : \n",
    "  \n",
    "  \n",
    "  first_term = (dataset_cleaned[i] - mean_x) ; \n",
    "\n",
    "  second_term = (dataset_cleaned[i] - mean_x) ; \n",
    "\n",
    "  variance += np.dot(first_term , second_term) ; \n",
    "  \n",
    "variance /= no_of_datapoint ; \n",
    "\n",
    "\n",
    "print(variance)\n",
    "\n",
    "# Thus we get the variance(sigma ^ 2) from this . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the question , we need to first see that the variance of the data is highly dominated by few features compared to other features . For this lets first print the individuals terms added by these features . ie. (xi - x_bar).T * (xi - x_bar) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.63662840e+01 9.59135541e+03 1.08743512e+03 6.74035926e+05\n",
      " 5.97715545e+00 1.81405896e-01 2.49552084e-01 1.72839506e-01\n",
      " 1.04700316e-02 7.87351978e-03 2.35827664e-01 1.63769211e-01\n",
      " 1.42388231e-01 5.01105792e-02 6.63265306e-02 6.40519582e-02\n",
      " 8.83023991e-02 6.63265306e-02 7.30662635e-02 7.96800202e-02\n",
      " 6.85871056e-02 8.61678005e-02 6.63265306e-02 6.40519582e-02\n",
      " 2.63850396e-03 7.08336833e-02 7.30662635e-02]\n",
      "\n",
      "Features contributing the most variance (in descending order) :  [ 3  1  2  0  4  6 10  5  7 11 12 16 21 19 26 18 25 20 22 17 14 23 15 13\n",
      "  8  9 24]\n"
     ]
    }
   ],
   "source": [
    "# Lets first find the contribution of the term (xi - x_bar).T * (xi - x_bar) from each feature . It is as follows : \n",
    "\n",
    "squared_diffs = np.zeros(no_of_features) ; \n",
    "\n",
    "for i in range(no_of_datapoint) : \n",
    "  diff = dataset_cleaned[i] - mean_x ; \n",
    "  \n",
    "  squared_diffs += diff ** 2 ; \n",
    "  \n",
    "\n",
    "variance = squared_diffs /no_of_datapoint ; \n",
    "\n",
    "\n",
    "print(variance) ; # now we are printing the variance array to check which term is more dominating , the feature which is more dominating . \n",
    "\n",
    "\n",
    "dominant_features = np.argsort(-variance) ;  \n",
    "\n",
    "print(f\"\\nFeatures contributing the most variance (in descending order) :  {dominant_features}\") ; \n",
    "\n",
    "# Thus we can see that features are contributing differently to the variance . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets normalize each feature with its mean and variance and then compute the variance normalized data . We use the formulae \n",
    "\n",
    "$$\n",
    "X_{normalised} = \\frac{X_{col} - \\mu_{col}}{\\sigma_{col}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.000000000000014\n"
     ]
    }
   ],
   "source": [
    "# we have mean for each feature in mean_X and variance of each feature in variance , thus we can proceed . \n",
    "\n",
    "normalized_data = np.zeros_like(dataset_cleaned) ; \n",
    "\n",
    "for i in range(no_of_datapoint): \n",
    "  normalized_data[i] = (dataset_cleaned[i] - mean_x) / (np.sqrt(variance)) ; \n",
    "\n",
    "# thus we normalized each datapoint in this \n",
    "\n",
    "normalized_variance = 0 ; \n",
    "\n",
    "for i in range(no_of_datapoint) : \n",
    "  \n",
    "  \n",
    "  normalized_variance += np.dot(normalized_data[i] , normalized_data[i]) ; \n",
    "  \n",
    "normalized_variance /= no_of_datapoint ; \n",
    "\n",
    "\n",
    "print(normalized_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we get variance as 27 as there are 27 number of columns and each column contributed equally to the variance . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the question , we need to check in the given dataset with a 5\\% level of significance , test if the number model year has any effect on the number of cylinders . We are also given reference to table for the same . Thus we are using cleaned data which no missing values and outliers . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using cleaned dataset for this purpose . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Chi-Squared test for finding the dependency of number of cylinders on model year . The hypothesis are as follows : \n",
    "\n",
    "## Hypotheses for Chi-Squared Test\n",
    "\n",
    "### Null Hypothesis (H0)\n",
    "The null hypothesis states that there is no association between the number of cylinders and the model year of the cars. In other words, the two variables are independent.\n",
    "\n",
    "**H₀**: The number of cylinders is independent of the model year.\n",
    "\n",
    "### Alternative Hypothesis (H1)\n",
    "The alternative hypothesis states that there is an association between the number of cylinders and the model year of the cars. This means that the two variables are dependent.\n",
    "\n",
    "**H₁**: The number of cylinders is dependent on the model year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table:\n",
      "col_0       model year_70.0  model year_71.0  model year_72.0  \\\n",
      "row_0                                                           \n",
      "cylinder_3                0                0                1   \n",
      "cylinder_4                7               12               13   \n",
      "cylinder_5                0                0                0   \n",
      "cylinder_6                4                8                0   \n",
      "cylinder_8               10                7               12   \n",
      "\n",
      "col_0       model year_73.0  model year_74.0  model year_75.0  \\\n",
      "row_0                                                           \n",
      "cylinder_3                1                0                0   \n",
      "cylinder_4               11               15               12   \n",
      "cylinder_5                0                0                0   \n",
      "cylinder_6                8                7               12   \n",
      "cylinder_8               17                5                6   \n",
      "\n",
      "col_0       model year_76.0  model year_77.0  model year_78.0  \\\n",
      "row_0                                                           \n",
      "cylinder_3                0                1                0   \n",
      "cylinder_4               14               14               17   \n",
      "cylinder_5                0                0                1   \n",
      "cylinder_6               10                5               12   \n",
      "cylinder_8                9                8                6   \n",
      "\n",
      "col_0       model year_79.0  model year_80.0  model year_81.0  \\\n",
      "row_0                                                           \n",
      "cylinder_3                0                1                0   \n",
      "cylinder_4               11               22               21   \n",
      "cylinder_5                1                1                0   \n",
      "cylinder_6                6                2                7   \n",
      "cylinder_8                9                0                1   \n",
      "\n",
      "col_0       model year_82.0  model year_99999.0  \n",
      "row_0                                            \n",
      "cylinder_3                0                   0  \n",
      "cylinder_4               27                   1  \n",
      "cylinder_5                0                   0  \n",
      "cylinder_6                3                   0  \n",
      "cylinder_8                0                   0  \n"
     ]
    }
   ],
   "source": [
    "column_names = [\n",
    "    'mpg', 'displacement', 'horsepower', 'weight', 'acceleration',\n",
    "    'cylinder_8', 'cylinder_4', 'cylinder_6', 'cylinder_3', 'cylinder_5',\n",
    "    'origin_1.0', 'origin_3.0', 'origin_2.0',\n",
    "    'model year_70.0', 'model year_71.0', 'model year_72.0',\n",
    "    'model year_73.0', 'model year_74.0', 'model year_75.0',\n",
    "    'model year_76.0', 'model year_77.0', 'model year_78.0',\n",
    "    'model year_79.0', 'model year_80.0', 'model year_99999.0',\n",
    "    'model year_81.0', 'model year_82.0'\n",
    "]\n",
    "\n",
    "dataset_chi = pd.DataFrame(dataset_cleaned , columns=column_names) ; \n",
    "\n",
    "\n",
    "\n",
    "cylinder_cols = dataset_chi.filter(like='cylinder_')\n",
    "model_year_cols = dataset_chi.filter(like='model year_')\n",
    "\n",
    "\n",
    "def get_max_column(df):\n",
    "    return df.idxmax(axis=1)\n",
    "\n",
    "cylinder_max = get_max_column(cylinder_cols)\n",
    "model_year_max = get_max_column(model_year_cols)\n",
    "\n",
    "\n",
    "contingency_table = pd.crosstab(cylinder_max, model_year_max)\n",
    "\n",
    "\n",
    "print(\"Contingency Table:\")\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula\n",
    "\n",
    "The Chi-Squared statistic is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\chi^2 \\) = Chi-Squared statistic\n",
    "- \\( O_i \\) = Observed frequency for each category\n",
    "- \\( E_i \\) = Expected frequency for each category, calculated as:\n",
    "\n",
    "$$\n",
    "E_i = \\frac{(Row \\, Total) \\times (Column \\, Total)}{Grand \\, Total}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be using Chi-test to test if the number model year has any effect on the number of cylinders . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.55958345492391\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "# We got the contigency table in the above step . \n",
    "observed_data = contingency_table.values ; \n",
    "\n",
    "# print(observed_data)\n",
    "row_sums = observed_data.sum(axis=1) ; \n",
    "\n",
    "column_sums = observed_data.sum(axis=0) ; \n",
    "\n",
    "total_sum = observed_data.sum() ; \n",
    "\n",
    "# print(row_sums) ; \n",
    "\n",
    "expected = np.outer(row_sums , column_sums ) / (total_sum) ; \n",
    "\n",
    "chi_squared_val = ((observed_data - expected) ** 2 / expected).sum() ; \n",
    "\n",
    "degree_of_freedom = (len(contingency_table.index) - 1) * (len(contingency_table.columns) - 1) ; \n",
    "\n",
    "print(chi_squared_val) ; \n",
    "\n",
    "print(degree_of_freedom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we get the chi_squared_value($\\chi^2$) as 101.56 . Now we know that the degree of freedom(dof) is (r-1) * (c-1) = 52 in this case as r = 5 and c = 14 . Now we know significance as 5 % . Thus it is a one sided test so the level of significance will remain as 5 % . Thus we can look in the table and answer the hypothesis . We can see that the threshold in the table is 73.810 . Thus ($\\chi^2$) > 69.832 . Thus we reject the null hypothesis that they are independent and thus conclude that number of cylinders are not independent of model years . "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
